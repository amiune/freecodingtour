{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f17742-abb6-4834-9ace-a82716ef2c01",
   "metadata": {},
   "source": [
    "# Vecinos Cercanos\n",
    "\n",
    "El algoritmo de los vecinos cercanos es probablemente el mas simple de entender pero tambien el mas usado (aunque en versiones mas complejas que la que veremos aqui)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5396f2-ed4e-4d83-8626-44261ad9dbe4",
   "metadata": {},
   "source": [
    "El algoritmo es el que explicamos con el ejemplo de la venta de un telefono movil:\n",
    "- Dada una tabla con datos de telefonos usados con sus precios y un nuevo telefono del cual quiero adivinar el precio busco cual de todos los telefonos de mi tabla se parece mas a este nuevo, es decir, busco el vecino mas cercano, y elijo el precio de este como el precio del nuevo telefono.\n",
    "- Una variacion es buscar dos o tres o mas vecinos cercanos y hacer un promedio de los precios.\n",
    "\n",
    "Para implementar este algoritmo bastan dos funciones:\n",
    "1. Una funcion que calcule la distancia entre dos datos (vectores), en este caso los telefonos.\n",
    "2. Una funcion de busqueda que recorra toda la tabla de entrenamiento y calcule las distancias, utilizando la funcion anterior, del dato nuevo a cada uno de los datos de la tabla de entrenamiento.\n",
    "\n",
    "Veamos una implementacion simple:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b579b-c2cd-4b56-98e3-94a9c514ba30",
   "metadata": {},
   "source": [
    "Implementemos primero la funcion de distancia. Para ello utilizaremos la super conocida funcion de distancia que se deduce del Teorema de Pitagoras:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"img/pitagoras.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Que cuando los datos (vectores) tienen mas de dos dimensiones se define de la siguiente manera:\n",
    "\n",
    "$$ P=(p_{1},p_{2},\\dots ,p_{n}) $$\n",
    "$$ Q=(q_{1},q_{2},\\dots ,q_{n}) $$\n",
    "\n",
    "$$ {d_{E}(P,Q)={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{n}-q_{n})^{2}}}={\\sqrt {\\sum _{i=1}^{n}(p_{i}-q_{i})^{2}}}.} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3889a5-3527-4726-ab28-255b0dc74e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distancia(P, Q):\n",
    "    d = 0\n",
    "    for i in range(len(P)):\n",
    "        d = d + (P[i] - Q[i])**2\n",
    "    return math.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6baacb-e46d-4be1-b1b6-09be10eece1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecinos_cercanos(X_entrenamiento, nuevo_dato):\n",
    "    min_d = 1000000000\n",
    "    vecino_mas_cercano = -1\n",
    "    for i in range(len(X_entrenamiento)):\n",
    "        P = X_entrenamiento.iloc[i,:].to_numpy()\n",
    "        Q = nuevo_dato.to_numpy()\n",
    "        d = distancia(P, Q)\n",
    "        print('Comparando nuevo dato con fila:',i)\n",
    "        print(P)\n",
    "        print(Q)\n",
    "        print(\"Distancia:\", d)\n",
    "        if d < min_d:\n",
    "            min_d = d\n",
    "            vecino_mas_cercano = i\n",
    "    return vecino_mas_cercano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e06728-1004-49d4-b366-c45027d6017e",
   "metadata": {},
   "source": [
    "Carguemos nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef539f-979d-4463-be1d-15ba3509593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tabla_telefonos2.csv\")\n",
    "X_entrenamiento = df.drop(\"Precio\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bab87f-acac-47d0-97bf-44049a3fe7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_telefono = pd.DataFrame({\n",
    "    'Bateria': [0.81],\n",
    "    'Memoria_64': [0],\n",
    "    'Memoria_256': [1],\n",
    "    'Memoria_512': [0],\n",
    "    'Color_Gris': [1],\n",
    "    'Color_Oro': [0],\n",
    "    'Color_Verde': [0],\n",
    "    'Estado_Bueno': [1],\n",
    "    'Estado_Muy bueno': [1],\n",
    "    'Estado_Reacondicionado': [1],\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9052ad-b31c-41ad-91fb-c5356b5f21fb",
   "metadata": {},
   "source": [
    "Probemos nuestra funcion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376143b-4b23-4d19-9b7b-62b829436332",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_vecino = vecinos_cercanos(X_entrenamiento, mi_telefono.iloc[0,:])\n",
    "print('El vecino mas cercano es el de la fila:', mejor_vecino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac934a9-ab16-467c-97a9-4429e638f8ea",
   "metadata": {},
   "source": [
    "El problema de este algoritmo es que al tener que comparar el nuevo dato con todos los anteriores es muy lento pero si guardamos los datos de maneras especiales entonces hay algoritmos mas eficientes que no necesitan ver todos los datos para encontrar los mas cercanos. Estos son unos de los algoritmos mas utilizados hoy en dia en las bases de datos vectoriales y se llaman algoritmos de aproximacion de vecinos cercanos (ANN por sus siglas en ingles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e89c7-4ac6-4725-851d-a3b721aa09c7",
   "metadata": {},
   "source": [
    "# Fin: [Volver al contenido del curso](https://www.freecodingtour.com/cursos/espanol/datascience/datascience.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
