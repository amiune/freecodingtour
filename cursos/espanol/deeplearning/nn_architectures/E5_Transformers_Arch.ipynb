{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b893fb7-d8f2-4098-98a7-2bb3f959fa85",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "Aqui proporciono enlaces para que los mas curiosos puedan entender en profundidad el funcionamiento de las redes de tipo transformer:\n",
    "\n",
    "Crea tu propio ChatGPT desde cero:\n",
    "\n",
    "- [Self Attention Layer Explanation by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n",
    "- [Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up)\n",
    "- [Build a Large Language Model (From Scratch)](https://sebastianraschka.com/llms-from-scratch/)\n",
    "- [Implement a transformer from scratch with pytorch by Andrej Karpathy (OpenAI, Stanford)](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "- [GPT in 60 Lines of NumPy by Jay Mody](https://jaykmody.com/blog/gpt-from-scratch/)\n",
    "- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Step by step without math by Miguel Grinberg](https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math)\n",
    "- [Original Attention Is All You Need paper with code](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "Modelos Multimodales:\n",
    "- [Understanding Multimodal LLMs by Sebastian Raschka](https://www.linkedin.com/pulse/understanding-multimodal-llms-sebastian-raschka-phd-t7h5c/)\n",
    "\n",
    "Visualiza como funciona ChatGPT:\n",
    "- [Transformer Visualization](https://bbycroft.net/llm)\n",
    "- [Attention Visualization](https://github.com/jessevig/bertviz)\n",
    "- [Transformer Explainer: Interactive Learning of Text-Generative Models](https://arxiv.org/abs/2408.04619)\n",
    "\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "Cursos Completos:\n",
    "\n",
    "- [Stanford CS336 (2025): Language Modeling from Scratch](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)\n",
    "- [Stanford CS336 (2024): Language Modeling from Scratch](https://stanford-cs336.github.io/spring2024/)\n",
    "- [Stanford CS224N (2025): Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D)\n",
    "- [Stanford CS224N (2024): Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)\n",
    "- [Deep NN course by Francois Fleuret](https://fleuret.org/dlc/)\n",
    "\n",
    "------------------------------\n",
    "\n",
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "------------------------------\n",
    "\n",
    "Tratando de entender que conceptos crea una red tipo transformer\n",
    "\n",
    "- [Mapping the Mind of a Large Language Model by Anthropic](https://www.anthropic.com/news/mapping-mind-language-model)\n",
    "- [Extracting Concepts from GPT-4](https://openai.com/index/extracting-concepts-from-gpt-4/)\n",
    "- [Tracing the thoughts of a large language model](https://www.anthropic.com/research/tracing-thoughts-language-model)\n",
    "- [Signs of introspection in large language models](https://www.anthropic.com/research/introspection)\n",
    "- [Why language models hallucinate](https://openai.com/index/why-language-models-hallucinate/)\n",
    "- [Understanding neural networks through sparse circuits](https://openai.com/index/understanding-neural-networks-through-sparse-circuits/)\n",
    "- [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)\n",
    "\n",
    "Tratando de evaluar LLMs:\n",
    "- [Measuring the performance of our models on real-world tasks](https://openai.com/index/gdpval/)\n",
    "- [DSPy: Programming—not prompting—Foundation Models](https://github.com/stanfordnlp/dspy)\n",
    "- [Defining and evaluating political bias in LLMs](https://openai.com/index/defining-and-evaluating-political-bias-in-llms/)\n",
    "\n",
    "The Q, K, V Matrices (Attention)\n",
    "- [\"Attention\", \"Transformers\", in Neural Network \"Large Language Models\"](http://bactra.org/notebooks/nn-attention-and-transformers.html)\n",
    "- [Attention video from U of Michigan](https://www.youtube.com/watch?v=YAgjfMR9R_M)\n",
    "- [The Q, K, V Matrices](https://arpitbhayani.me/blogs/qkv-matrices/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894a4c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee897cd3-4e8b-484b-b2c1-d32b87cd1975",
   "metadata": {},
   "source": [
    "## Redes tipo Transformer en la Practica\n",
    "\n",
    "Aqui he creado tutoriales para hacer finetuning de redes neuronales tipo transformers y otros usos practicos:\n",
    "\n",
    "- [Transformer Finetuning con Pytorch](https://colab.research.google.com/github/amiune/freecodingtour/blob/main/cursos/espanol/deeplearning/nn_architectures/Transformer_Finetuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb781c5-721d-4954-a1e2-8b15397bde41",
   "metadata": {},
   "source": [
    "# Fin: [Volver al contenido del curso](https://www.freecodingtour.com/cursos/espanol/deeplearning/deeplearning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
