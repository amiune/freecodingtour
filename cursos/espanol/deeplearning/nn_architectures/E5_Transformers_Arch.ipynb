{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b893fb7-d8f2-4098-98a7-2bb3f959fa85",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "Aqui proporciono enlaces para que los mas curiosos puedan entender en profundidad el funcionamiento de las redes de tipo transformer:\n",
    "\n",
    "Crea tu propio ChatGPT desde cero:\n",
    "\n",
    "- [Self Attention Layer Explanation by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)\n",
    "- [Building LLMs from the Ground Up: A 3-hour Coding Workshop by Sebastian Raschka](https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up)\n",
    "- [Implement a transformer from scratch with pytorch by Andrej Karpathy (OpenAI, Stanford)](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "- [GPT in 60 Lines of NumPy by Jay Mody](https://jaykmody.com/blog/gpt-from-scratch/)\n",
    "- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Step by step without math by Miguel Grinberg](https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math)\n",
    "- [Original Attention Is All You Need paper with code](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "Visualiza como funciona ChatGPT:\n",
    "- [Transformer Visualization](https://bbycroft.net/llm)\n",
    "- [Attention Visualization](https://github.com/jessevig/bertviz)\n",
    "- [Transformer Explainer: Interactive Learning of Text-Generative Models](https://arxiv.org/abs/2408.04619)\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "Cursos Completos:\n",
    "\n",
    "- [Deep NN course by Francois Fleuret](https://fleuret.org/dlc/)\n",
    "- [CS224N: Natural Language Processing with Deep Learning\n",
    "Stanford / Spring 2024](https://web.stanford.edu/class/cs224n/)\n",
    "- [Stanford CS224N: Natural Language Processing with Deep Learning | 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)\n",
    "- [Stanford CS224N NLP with Deep Learning | 2023](https://www.youtube.com/playlist?list=PL613dYIGMXoZ0Wl6tj8VvHaFUTAWE8fbW)\n",
    "\n",
    "------------------------------\n",
    "\n",
    "Tratando de entender que conceptos crea una red tipo transformer\n",
    "\n",
    "- [Mapping the Mind of a Large Language Model by Anthropic](https://www.anthropic.com/news/mapping-mind-language-model)\n",
    "- [Extracting Concepts from GPT-4\n",
    "](https://openai.com/index/extracting-concepts-from-gpt-4/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee897cd3-4e8b-484b-b2c1-d32b87cd1975",
   "metadata": {},
   "source": [
    "## Redes tipo Transformer en la Practica\n",
    "\n",
    "Aqui he creado tutoriales para hacer finetuning de redes neuronales tipo transformers y otros usos practicos:\n",
    "\n",
    "- [Transformer Finetuning con Pytorch](https://colab.research.google.com/github/amiune/freecodingtour/blob/main/cursos/espanol/deeplearning/nn_architectures/Transformer_Finetuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb781c5-721d-4954-a1e2-8b15397bde41",
   "metadata": {},
   "source": [
    "# Fin: [Volver al contenido del curso](https://www.freecodingtour.com/cursos/espanol/deeplearning/deeplearning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
