{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2efdf0-8ee2-4f99-a0eb-2eb7c2edd822",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "Como vimos anteriormente podemos utilizar estos Large Language Models entrenados para hacerles preguntas y recibir sus respuestas.\n",
    "\n",
    "Es como si tuvieramos una funcion que recibe un texto como entrada y devuelve un texto como salida. Donde el texto de entrada puede ser cualquier pregunta y el texto de respuesta sera esta respuesta magica que es capaz de darnos un LLM. \n",
    "\n",
    "Esto parece simple y tonto pero es el sue√±o de cualquier programador. Es como si fuera posible hacer una busqueda en google y luego poder hacer web scrapping del mejor resultado pero de manera mucho mas sencilla.\n",
    "\n",
    "LangChain es una API que nos permite realizar estas consultas a los LLM y poder parsear el resultado de forma mas simple. Veamos la diferencia entre consultar directamente a OpenAI como hicimos en la unidad anterior y hacerlo utilizando LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f2e42-9ea5-4bae-a378-7261ee68406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai --upgrade --quiet\n",
    "!pip install langchain --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19ca64-06f3-419c-9be0-61d5862812a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Cargar token desde google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "with open('drive/MyDrive/Tokens/openai.txt','r') as f:\n",
    "    token = f.read()\n",
    "    openai.api_key = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b86c2-43e4-4927-be38-81eb68826c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=token, temperature=0.0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96580451-87f5-4029-88d7-168a9f64f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Traduce el siguiente texto \\\n",
    "que esta delimitado por comillas triples \\\n",
    "al siguiente estilo {estilo}. \\\n",
    "texto: ```{texto}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "message = prompt_template.format_messages(\n",
    "                    estilo=\"rapero\",\n",
    "                    texto=\"Estimado Carlos, hoy no podre ir a tomar \\\n",
    "                    el te a tu casa porque estoy resfriado.\")\n",
    "\n",
    "response = chat(message)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6d859-ee5c-4755-bf60-e239be536589",
   "metadata": {},
   "source": [
    "## Parsear la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284fbd6-4b24-4b1d-8239-2720d59bac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1bdb3-8d7f-4d65-98c9-e1672883619d",
   "metadata": {},
   "source": [
    "### Genero el schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ba52a-3b3c-41ca-887b-09546f1d6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_schema = ResponseSchema(name=\"precio\",\n",
    "                             description=\"Cual ha sido el precio pagado?\\\n",
    "                             Si no se encuentra la informacion devolver -1.\")\n",
    "calificacion_schema = ResponseSchema(name=\"calificacion\",\n",
    "                                      description=\"Que calificacion se le asigno al restaurante?\\\n",
    "                                      Si no se encuentra la informacion devolver -1.\")\n",
    "\n",
    "response_schemas = [precio_schema, \n",
    "                    calificacion_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0a7b8-bede-426e-bff3-88009afa1742",
   "metadata": {},
   "source": [
    "### Consulto al LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65672fb9-4f82-48f0-bc3a-9e7940593983",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "La comida fue espectacular, lo mejor la tortilla de patatas \\\n",
    "y la tarta de queso. El precio bien, en total costo 23 euros.\n",
    "Le doy una calificacion de 8.3.\n",
    "\"\"\"\n",
    "\n",
    "review_template_2 = \"\"\"\\\n",
    "Para el siguiente texto obtener la siguiente informacion:\n",
    "\n",
    "precio: Cual ha sido el precio pagado?\\\n",
    "Si no se encuentra la informacion devolver -1.\n",
    "\n",
    "calificacion: Que calificacion se le asigno al restaurante?\\\n",
    "Si no se encuentra la informacion devolver -1.\n",
    "\n",
    "texto: {texto}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9c945-8de6-4a81-a002-aade247f8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b337c1-2b98-41b5-8563-31d9d9a67a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b977-5fb1-4eb8-be2e-5aa9f6b7fd57",
   "metadata": {},
   "source": [
    "## Memoria\n",
    "\n",
    "Los modelos generalmente estan hechos de forma que no recuerdan lo que les has dicho anteriormente. Para solucionar esto podemos ir guardando todo lo que vamos \"conversando\" con el modelo y pasarselo como entrada junto con la nueva pregunta o interaccion que queremos tener con el modelo. Veamos como LangChain nos permite hacer esto de forma sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90afd-8109-4b20-8aa3-a29255180ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef883f-3f97-4f58-bb44-08d4b3a10f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=token, temperature=0.0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ada612-0955-460f-baa5-d40b3c5f90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hola mi nombre es Hernan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba679e9-fce4-477b-a789-446f7a598c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Cuanto es 20 + 22?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36266df1-f138-41f8-98ab-b3f04a49b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Cual es mi nombre?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0624d-bcae-4178-bc57-12a50bdcb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de0f05-f83e-446d-a067-b9113fabb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a2b03-0ddc-4782-80c3-665cb9a06dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406cfc4-a575-4c50-a975-3128d1251d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hola\"}, \n",
    "                    {\"output\": \"Como estas?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b61c1-c6e8-4a89-b7dc-63f1c3007daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4116e-cce7-40f7-9607-7dae523d66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e804fa6-9dcc-4bb6-b3a2-3eb219983c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Bien, descansando\"}, \n",
    "                    {\"output\": \"Que bueno\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f67397-f19a-4bc7-9497-aaa1845730c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07468dad-3b27-447a-9acd-19b7da2d686c",
   "metadata": {},
   "source": [
    "## Limitar la memoria\n",
    "\n",
    "Si la conversacion es muy larga la memoria puede crecer demasiado, para limitar la cantidad de memoria que queremos que el modelo tenga LangChain nos ofrece diferentes formas de hacerlo:\n",
    "- WindowMemory: guarda las k ultimas preguntas junto con sus respectivas respuestas\n",
    "- TokenBufferMemory: guarda los ultimos max_token_limit tokens\n",
    "- ConversationSummaryBufferMemory: guarda un resumen de max_token_limit tokens de lo conversado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba65098-df7e-4424-be7c-e3a08ac4efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344c9fc-5018-4924-ba37-8547a832c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301992a-f706-49ac-963c-d1c9d9ba55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150049ac-d851-49b9-8727-051927f0c8d1",
   "metadata": {},
   "source": [
    "## Referencias:\n",
    "Esta unidad es un resumen traducido con algunas modificaciones del [curso de Andrew Ng de Deeplearning.ai](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n",
    "\n",
    "- https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a85135-9226-4042-aaff-f26105f5ef62",
   "metadata": {},
   "source": [
    "# Fin: [Volver al contenido del curso](https://www.freecodingtour.com/cursos/espanol/deeplearning/deeplearning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
