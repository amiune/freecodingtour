{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355a45d0-f340-4a55-8249-0a86a4f06bec",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "El fine tuning de una red neuronal consiste en ajustar sus la salida de la red y sus parametros ya preentrenados con una base de datos mas peque√±a y especifica que la utilizada para el entrenamiento de base.\n",
    "\n",
    "Como realizar el finetuning depende del modelo elegido. A continuacion veremos como realizar finetuning de un modelo con arquitectura ViT (Vision Transformer) para clasificacion de imagenes y en las proximas unidades veremos como realizar finetuning de un modelo Transformer para clasificacion de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02a872-e59c-4e8b-9b27-4db53cbed1e6",
   "metadata": {},
   "source": [
    "## Fine Tuning de Modelos Transformers con Huggingface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7e794-ed70-4e05-b09a-6e73fd91c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffc0ec-8dba-498c-99c7-feaade212ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f8069-b6bd-4c77-a0a7-205a30f92654",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/datasets\n",
    "https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615cf9c-a454-4619-a626-4dec36d2364e",
   "metadata": {},
   "source": [
    "Seleccionemos un dataset para clasificacion de imagenes:\n",
    "- [Analisis de emociones 11MB](https://huggingface.co/datasets/FastJobs/Visual_Emotional_Analysis)\n",
    "- [Snaks 110MB](https://huggingface.co/datasets/Matthijs/snacks)\n",
    "- [Flores 347MB](https://huggingface.co/datasets/nelorth/oxford-flowers)\n",
    "- [RayosX Pulmones 203MB](https://huggingface.co/datasets/keremberke/chest-xray-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796b1de-26b3-45ab-8033-9e5e5a6b9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('FastJobs/Visual_Emotional_Analysis', split='train')\n",
    "ds = ds.train_test_split(test_size=0.2, shuffle=True, stratify_by_column='label')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb17392-d67c-4fb2-a3d3-b302df4dd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_ejemplo = ds['train'][0]\n",
    "fila_ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6832292-91e0-4e29-87fa-3f32093f1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_ejemplo['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73fba1-90f1-47ab-847a-ed406e125f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_ejemplo['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011441a-4100-4627-9e8b-1ccf3cfec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ds['train'].features['label']\n",
    "labels.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4ca25-429f-4477-b598-8f1357c59071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "\n",
    "def show_examples(ds, seed=42, examples_per_class=3, size=(90, 90)):\n",
    "\n",
    "    w, h = size\n",
    "    labels = ds['train'].features['label'].names\n",
    "    grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
    "    draw = ImageDraw.Draw(grid)\n",
    "\n",
    "    for label_id, label in enumerate(labels):\n",
    "\n",
    "        # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
    "        ds_slice = ds['train'].filter(lambda ex: ex['label'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
    "\n",
    "        # Plot this label's examples along a row\n",
    "        for i, example in enumerate(ds_slice):\n",
    "            image = example['image']\n",
    "            idx = examples_per_class * label_id + i\n",
    "            box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
    "            grid.paste(image.resize(size), box=box)\n",
    "            draw.text(box, label, (255, 255, 255))\n",
    "\n",
    "    return grid\n",
    "\n",
    "show_examples(ds, seed=random.randint(0, 1337), examples_per_class=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac8134-6b2c-4829-9905-d0a08dd214ca",
   "metadata": {},
   "source": [
    "## Convertir la imagen a la entrada que necesita el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfdff3-e151-4efe-a9ac-9e35be5f1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6067f7-60af-4a41-93f1-905529b48198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea56733-ef6e-43be-b964-1900863e06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_example(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da77d32-1a6c-47a6-af5f-8dae4559e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt').to(device)\n",
    "    inputs['labels'] = torch.tensor(example_batch['label']).to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104484db-50ce-4296-8582-84c494699232",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b869e-a166-49a1-90fa-10b2007ff0ec",
   "metadata": {},
   "source": [
    "## Create la funcion para darle de comer al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefc613-d8cb-4cd2-8dd8-017a1d814ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]).to(device),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch]).to(device)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e0201-8ced-4f91-9bcc-3309fdb22557",
   "metadata": {},
   "source": [
    "## Elegir la metrica para evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8072f3-d865-4d56-9f0b-e4f7555ce1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688f59e-9b16-4cf1-866e-3485abbbc5d8",
   "metadata": {},
   "source": [
    "## Cargar el modelo preentrenado para clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563433f-e9f4-4611-93ae-60afd2935267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = ds['train'].features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf77d08-df5c-459e-bad8-da1a75b0d92f",
   "metadata": {},
   "source": [
    "## Elegir los parametros para configurar como hacer el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67789f7a-6d6b-4932-9e1e-03d154f9b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-emotions\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26945f-f694-4a4a-8305-0d856e8f9a01",
   "metadata": {},
   "source": [
    "## Crear el objeto para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d18cb-f96f-4b27-bd5d-0c6fa986e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19877b91-0beb-473d-9174-56b14e4fbe5c",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fb109-5eb1-4264-b289-7122ebd55b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f2aba-817e-4880-92bf-fbcf00345e56",
   "metadata": {},
   "source": [
    "## Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2995fa-08e9-401f-8f40-ff53444b23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b18c12-f83a-4082-90b0-d44d96d967ed",
   "metadata": {},
   "source": [
    "## Clasificar una imagen nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85342663-5f23-4bb0-9dd0-9b47b059c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get('https://images.unsplash.com/photo-1494790108377-be9c29b29330')\n",
    "image = Image.open(BytesIO(response.content))\n",
    "image.thumbnail((600,300),Image.LANCZOS)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f03280-51f2-4f2f-b2f4-143690ba759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = processor(image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = my_input.pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb5018-db41-4074-89bd-3752b79f8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values)\n",
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113bf9f-7a28-45f0-b11a-4dc9e71177a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = logits.argmax(-1)\n",
    "labels.names[prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac482441-395f-4d23-a374-8a278a5b9bd0",
   "metadata": {},
   "source": [
    "#### Referencias:\n",
    "\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer\n",
    "- https://huggingface.co/blog/fine-tune-vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94244657-107f-4e36-9d08-88ba57823f71",
   "metadata": {},
   "source": [
    "# Fin: [Volver al contenido del curso](https://www.freecodingtour.com/cursos/espanol/deeplearning/deeplearning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
