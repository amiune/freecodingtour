{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6d1e87",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "En este caso la entrada sera el mundo, o el medioambiente (enviroment en ingles) y la salida sera una accion de un robot ya sea virtual o fisico al que llamaremos agente (agent en ingles).\n",
    "\n",
    "Analicemos como pueden ser cada uno de estos elementos, la entrada, el proceso y la salida:\n",
    "\n",
    "## La entrada\n",
    "\n",
    "Como dijimos la entrada es el mundo, en el caso mas complejo la entrada seran imagenes, sonidos, olores, datos de sensores tactiles y cualquier otro tipo de sensor que nos imaginemos para obtener la mayor informacion posible del mundo que nos rodea (environment).\n",
    "\n",
    "Comenzaremos con ejemplos de mundos muy simples que van a ser tableros de 3x3 de un juego de laberinto donde hay que encontrar la salida. Luego iremos complejizando el mundo incorporante adversarios y objetos que aparecen que no conociamos y que veremos por primera vez mientras jugamos.\n",
    "\n",
    "## El algoritmo de aprendizaje\n",
    "\n",
    "En casos simples donde la cantidad de estados del environment sea peque√±a y los conozcamos de antemano podemos utilizar algoritmos de busqueda como DFS o similares para obtener una buena solucion pero ese no es el objetivo del curso. El objetivo es aprender a partir de la exploracion del environment y el metodo de prueba y error. Para ello existen varios algoritmos aunque aqui nos centraremos en el algoritmo llamado Q Learning que crea una tabla donde las filas representan los estados del environment y las columnas las posibles acciones del agente y cada celda el valor de ejecutar cada accion en cada estado. Luego pasaremos a Deep Q Learning cuando los estados y las acciones sean demasiados para ponerlos todos en una tabla.\n",
    "\n",
    "## La salida\n",
    "\n",
    "La salida sera la accion que debera hacer el Agente, el cual si es un robot sera el movimiento que tiene que realizar dicho robot. Dependiendo de su construccion ejecutaremos la rotacion de un motor o la activacion de un dispositivo hidraulico o de otro tipo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e414c",
   "metadata": {},
   "source": [
    "# Referencias:\n",
    "\n",
    "### Basic\n",
    "- [Kaggle RL Course](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)\n",
    "- [HuggingFace RL Course](https://huggingface.co/learn/deep-rl-course/)\n",
    "\n",
    "### Advanced\n",
    "- [Book: Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- [Notes: Professor Bruno C. da Silva University of Massachusetts Amherst](https://people.cs.umass.edu/~bsilva/courses/CMPSCI_687/Fall2022/Lecture_Notes_v1.0_687_F22.pdf)\n",
    "- [OpenAI RL educational resource](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "- [Stanford Course](https://web.stanford.edu/class/cs234/index.html) - [[Videos]](https://www.youtube.com/watch?v=E3f2Camj0Is&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) - [[Code]](https://github.com/tallamjr/stanford-cs234/tree/master)\n",
    "- [Bevan Smith Intro to RL videos](https://www.youtube.com/watch?v=aldWdgUaYRA&list=PLuVnJWOCzbxXcpYnL4WCxRwp8kqHgBoIF)\n",
    "- [Deep Mind RL Video Series 1](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) and [slides](https://www.davidsilver.uk/teaching/)\n",
    "- [Deep Mind RL Video Series 2](https://www.youtube.com/playlist?list=PLqYmG7hTraZCRwoyGxvQkqVrZgDQi4m-5)\n",
    "- [Temporal Difference Learning by Rich Sutton video series](https://videolectures.net/deeplearning2017_sutton_td_learning/)\n",
    "- [More](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning/discussion/499257)\n",
    "- [Another AI Stanford Course](https://www.youtube.com/watch?v=ZiwogMtbjr4&list=PLoROMvodv4rOca_Ovz1DvdtWuz8BfSWL2)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "---------------------\n",
    "Simple Q learning implementations:\n",
    "- [Tic Tac Toe](https://colab.research.google.com/drive/1p1Fdhk0NzWNkogy2KjMz2MV2_yMxV_Bm#scrollTo=Z2uk3Aa8zISC)\n",
    "- [Maze](https://github.com/abhijitmajumdar/Simple_Q_Learning)\n",
    "\n",
    "---------------------\n",
    "Hugging Face RL Course\n",
    "\n",
    "- [HF Certification Progress](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)\n",
    "- [Certification Process](https://huggingface.co/learn/deep-rl-course/en/communication/certification)\n",
    "- [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)\n",
    "- [AI vs AI](https://huggingface.co/learn/deep-rl-course/unit7/introduction?fw=pt)\n",
    "\n",
    "-----\n",
    "\n",
    "Cool Papers:\n",
    "\n",
    "- [Aloha](https://mobile-aloha.github.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
