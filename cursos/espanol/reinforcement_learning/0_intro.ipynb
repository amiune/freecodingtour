{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6d1e87",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "En este caso la entrada sera el mundo, o el entorno (enviroment en ingles) y la salida sera una accion de un robot ya sea virtual o fisico al que llamaremos agente (agent en ingles).\n",
    "\n",
    "Analicemos como pueden ser cada uno de estos elementos, la entrada, el proceso y la salida:\n",
    "\n",
    "## La entrada\n",
    "\n",
    "Como dijimos la entrada es el mundo, en el caso mas complejo la entrada seran imagenes, sonidos, olores, datos de sensores tactiles y cualquier otro tipo de sensor que nos imaginemos para obtener la mayor informacion posible del mundo que nos rodea (environment).\n",
    "\n",
    "Comenzaremos con ejemplos de mundos muy simples que van a ser tableros de 3x3 de un juego de laberinto donde hay que encontrar la salida. Luego iremos complejizando el mundo incorporando adversarios y objetos que aparecen que no conociamos y que veremos por primera vez mientras jugamos.\n",
    "\n",
    "## El algoritmo de aprendizaje\n",
    "\n",
    "En casos simples donde la cantidad de estados del environment sea pequeña y los conozcamos de antemano podemos utilizar algoritmos de busqueda como DFS o similares para obtener una buena solucion pero ese no es el objetivo del curso. El objetivo es aprender a partir de la exploracion del environment y el metodo de prueba y error. Para ello existen varios algoritmos aunque aqui nos centraremos en el algoritmo llamado Q Learning que crea una tabla donde las filas representan los estados del environment y las columnas las posibles acciones del agente y cada celda el valor de ejecutar cada accion en cada estado. Luego pasaremos a Deep Q Learning cuando los estados y las acciones sean demasiados para ponerlos todos en una tabla.\n",
    "\n",
    "## La salida\n",
    "\n",
    "La salida sera la accion que debera hacer el Agente, el cual si es un robot sera el movimiento que tiene que realizar dicho robot. Dependiendo de su construccion ejecutaremos la rotacion de un motor o la activacion de un dispositivo hidraulico o de otro tipo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e414c",
   "metadata": {},
   "source": [
    "# Referencias:\n",
    "\n",
    "## Home Made Reinforcement Learning:\n",
    "\n",
    "- [Unit 1: Creating the environmet](HomeMadeRL_1_Env_v1_solution.ipynb)\n",
    "- [Unit 2: Creating a Dummy Agent](HomeMadeRL_2_Agent_Dummy_v1_solution.ipynb)\n",
    "- [Unit 3: Creating a QLearning Agent](HomeMadeRL_3_Agent_QLearning_v1_solution.ipynb)\n",
    "- [Unit 4: Creating a Deep QLearning Agent with sklearn](HomeMadeRL_4_Agent_DeepQLearning_v1_solution.ipynb)\n",
    "- [Unit 4.1: Adding Memory Replay](HomeMadeRL_4.1_Agent_DeepQLearning_MemoryReplay_v1_solution.ipynb)\n",
    "- [Unit 4.2: Converting from sklearn to pytorch](HomeMadeRL_4.2_Agent_DeepQLearning_MemoryReplay_Pytorch_v1_solution.ipynb)\n",
    "- [Unit 5: Making the Environment compatible with Gymnasium](HomeMadeRL_5_GymEnv_StableBaselines3_v1_solution.ipynb)\n",
    "\n",
    "\n",
    "### Ref:\n",
    "\n",
    "- [Gymnasium is a maintained fork of OpenAI’s Gym library. The Gymnasium interface is simple, pythonic, and capable of representing general RL problems](https://gymnasium.farama.org)\n",
    "- [Stable-Baselines3 Docs - Reliable Reinforcement Learning Implementations](https://stable-baselines3.readthedocs.io/en/master/index.html)\n",
    "- [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)\n",
    "\n",
    "-----\n",
    "\n",
    "### Basic\n",
    "- [Kaggle RL Course](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)\n",
    "- [HuggingFace RL Course](https://huggingface.co/learn/deep-rl-course/)\n",
    "\n",
    "### Advanced\n",
    "- [Book: Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- [Notes: Professor Bruno C. da Silva University of Massachusetts Amherst](https://people.cs.umass.edu/~bsilva/courses/CMPSCI_687/Fall2022/Lecture_Notes_v1.0_687_F22.pdf)\n",
    "- [OpenAI RL educational resource](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "- [Multi-Agent Reinforcement Learning: Foundations and Modern Approaches](https://www.marl-book.com)\n",
    "- [Stanford Course](https://web.stanford.edu/class/cs234/index.html) - [[Videos]](https://www.youtube.com/watch?v=E3f2Camj0Is&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) - [[Code]](https://github.com/tallamjr/stanford-cs234/tree/master)\n",
    "- [Theoretical Foundations of Reinforcement Learning at Univertisy of Alberta](https://rltheory.github.io)\n",
    "- [Robotics at Michigan University](https://robotics.umich.edu/academics/courses/online-courses/)\n",
    "- [Deep Mind RL Video Series 1](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) and [slides](https://www.davidsilver.uk/teaching/)\n",
    "- [Deep Mind RL Video Series 2](https://www.youtube.com/playlist?list=PLqYmG7hTraZCRwoyGxvQkqVrZgDQi4m-5)\n",
    "- [Temporal Difference Learning by Rich Sutton video series](https://videolectures.net/deeplearning2017_sutton_td_learning/)\n",
    "- [Another AI Stanford Course](https://www.youtube.com/watch?v=ZiwogMtbjr4&list=PLoROMvodv4rOca_Ovz1DvdtWuz8BfSWL2)\n",
    "\n",
    "-----\n",
    "\n",
    "### Hugging Face RL Course\n",
    "\n",
    "- [HF Certification Progress](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)\n",
    "- [Certification Process](https://huggingface.co/learn/deep-rl-course/en/communication/certification)\n",
    "- [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)\n",
    "- [AI vs AI](https://huggingface.co/learn/deep-rl-course/unit7/introduction?fw=pt)\n",
    "- [Stable Baselines3 Zoo](https://rl-baselines3-zoo.readthedocs.io/en/master/index.html)\n",
    "\n",
    "-----\n",
    "\n",
    "### RLHF\n",
    "- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    "- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)\n",
    "\n",
    "-----\n",
    "\n",
    "### Parameter Optimization:\n",
    "- [Optuna](https://optuna.org/#code_examples)\n",
    "- [Automatic Hyperparameter Optimization](https://www.youtube.com/watch?v=AidFTOdGNFQ)\n",
    "- [Hyperparameter Tuning with Optuna Notebook](https://www.youtube.com/watch?v=ihP7E76KGOI)\n",
    "\n",
    "-----\n",
    "\n",
    "### Open Spiel\n",
    "- [Open Spiel Github](https://github.com/google-deepmind/open_spiel/tree/master)\n",
    "\n",
    "### Unity MLAgents\n",
    "- [Install Unity Hub and Editor](https://learn.unity.com/tutorial/install-the-unity-hub-and-editor?uv=2022.3)\n",
    "- [Unity Basics](https://learn.unity.com/tutorial/66f2e7e9edbc2a01255f7970?uv=6&pathwayId=664b6225edbc2a01973f4f19&missionId=664bdda6edbc2a09177bccae#)\n",
    "- [Unity Game Creation](https://learn.unity.com/course/create-with-code?uv=2022.3)\n",
    "- [Create a video game with Unity](https://learn.unity.com/course/create-with-code)\n",
    "- [MLAgents Instalation](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Installation.md)\n",
    "- [Unity MLAgents Intro](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)\n",
    "- [Unity MLAgents Docs](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md)\n",
    "\n",
    "#### Soccer Twos\n",
    "\n",
    "- [HF Model Test](https://unity-ml-agents-soccertwos.static.hf.space/index.html)\n",
    "- [Bryan Oliveira University Goias](https://bryanoliveira.github.io/blog/2021-09-05-soccer-twos-env/)\n",
    "- [Gym Evn](https://github.com/bryanoliveira/soccer-twos-env)\n",
    "\n",
    "-----\n",
    "\n",
    "## Robots\n",
    "\n",
    "- [ROS](https://www.ros.org)\n",
    "- [MuJoCo](https://mujoco.readthedocs.io/en/stable/overview.html)\n",
    "- [Genesis](https://genesis-world.readthedocs.io/en/latest/)\n",
    "- [NVIDIA Isacc](https://developer.nvidia.com/isaac/sim)\n",
    "- [Gymnasium Robotics](https://robotics.farama.org)\n",
    "- [panda-gym](https://github.com/qgallouedec/panda-gym)\n",
    "\n",
    "### LeRobot\n",
    "- [LeRobot](https://github.com/huggingface/lerobot)\n",
    "\n",
    "### SO100\n",
    "- [SO-100](https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md)\n",
    "- [How to use the SO-ARM100 robotic arm in Lerobot](https://wiki.seeedstudio.com/lerobot_so100m/)\n",
    "- [How to Import and Control SO100Arm Kit in Isaac Sim](https://wiki.seeedstudio.com/lerobot_so100m_isaacsim/)\n",
    "- [Buy Assembled](https://shop.wowrobo.com/products/so-arm100-diy-kit-assembled-version?variant=46419129762009)\n",
    "- [Buy fancy version](https://robots.phospho.ai)\n",
    "\n",
    "#### Feetech Servos\n",
    "- [feetech servos sdk](https://github.com/Adam-Software/Feetech-Servo-SDK/tree/main)\n",
    "- [feetech examples](https://github.com/hansonrobotics/servo_experiments)\n",
    "- [lerobot feetech class](https://github.com/huggingface/lerobot/blob/main/lerobot/common/robot_devices/motors/feetech.py)\n",
    "  \n",
    "-----\n",
    "\n",
    "#### Cool Papers:\n",
    "\n",
    "- [Aloha](https://tonyzhaozh.github.io/aloha/) -> [Mobile Aloha](https://mobile-aloha.github.io) -> [Gymnasium Environment](https://github.com/huggingface/gym-aloha)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
