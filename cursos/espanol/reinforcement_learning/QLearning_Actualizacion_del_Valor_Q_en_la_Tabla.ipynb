{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actualización del Valor Q"
      ],
      "metadata": {
        "id": "FP6V4Z79rJCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo Generico\n",
        "\n",
        "La actualización del valor Q es el núcleo del algoritmo de Q-Learning. Este paso ajusta los valores en la tabla de Q-Learning según la recompensa obtenida y la estimación del mejor resultado futuro. Utiliza la siguiente fórmula:\n",
        "\n",
        "$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$\n",
        "\n",
        "Donde:\n",
        "- $ Q(s, a) $: Valor Q actual para el estado $s$ y la acción $a$.\n",
        "- $\\alpha $: Tasa de aprendizaje (controla cuánto se ajustan los valores).\n",
        "- $ r $: Recompensa inmediata obtenida tras tomar la acción $a$ desde el estado $s$.\n",
        "- $\\gamma $: Factor de descuento (pondera la importancia de recompensas futuras).\n",
        "- $ \\max_{a'} Q(s', a') $: Máximo valor Q posible desde el próximo estado $s'$.\n",
        "- $ Q(s, a) \\leftarrow $: Actualización del valor Q.\n",
        "\n",
        "---\n",
        "\n",
        "### **Ejemplo Simple**\n",
        "\n",
        "Supongamos un agente en un entorno de 4 estados $ S = \\{A, B, C, D\\} $ y 2 acciones posibles $ A = \\{1, 2\\} $.\n",
        "\n",
        "**Paso 1: Configuración Inicial**\n",
        "1. Inicializamos la tabla Q con valores de cero:\n",
        "\n",
        "   \n",
        "   | Estado | Acción 1 | Acción 2 |\n",
        "   |--------|----------|----------|\n",
        "   | A      | 0        | 0        |\n",
        "   | B      | 0        | 0        |\n",
        "   | C      | 0        | 0        |\n",
        "   | D      | 0        | 0        |\n",
        "\n",
        "**Paso 2: Iteración del agente**\n",
        "1. El agente comienza en el estado $ A $ y decide tomar la acción $ 1 $.\n",
        "2. Tras la acción, el agente se mueve al estado $ B $ y recibe una recompensa inmediata de $ r = 5 $.\n",
        "\n",
        "**Paso 3: Actualización del valor $ Q(A, 1) $**\n",
        "1. Supongamos que la tasa de aprendizaje es $ \\alpha = 0.1 $ y el factor de descuento es $ \\gamma = 0.9 $.\n",
        "2. El valor máximo $ Q(B, a') $ para el estado $ B $ se calcula. Como todos los valores Q son $ 0 $ inicialmente, $ \\max_{a'} Q(B, a') = 0 $.\n",
        "3. Sustituimos en la fórmula:\n",
        "\n",
        "$\n",
        "Q(A, 1) \\leftarrow Q(A, 1) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(B, a') - Q(A, 1) \\right]\n",
        "$\n",
        "\n",
        "$\n",
        "Q(A, 1) \\leftarrow 0 + 0.1 \\left[ 5 + 0.9 \\cdot 0 - 0 \\right]\n",
        "$\n",
        "\n",
        "$\n",
        "Q(A, 1) \\leftarrow 0.1 \\cdot 5 = 0.5\n",
        "$\n",
        "\n",
        "**Paso 4: Actualizar la tabla**\n",
        "La tabla de valores Q queda ahora:\n",
        "\n",
        "   | Estado | Acción 1 | Acción 2 |\n",
        "   |--------|----------|----------|\n",
        "   | A      | **0.5**  | 0        |\n",
        "   | B      | 0        | 0        |\n",
        "   | C      | 0        | 0        |\n",
        "   | D      | 0        | 0        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretación**\n",
        "- El valor $ Q(A, 1) = 0.5 $ indica que tomar la acción $ 1 $ desde el estado $ A $ parece ser una buena decisión, dado el historial de recompensas hasta ahora.\n",
        "- En iteraciones posteriores, el agente ajustará más valores en la tabla según las recompensas obtenidas y las estimaciones futuras.\n",
        "\n",
        "Este proceso se repite durante varias iteraciones hasta que la tabla Q converge, es decir, los valores reflejan la estrategia óptima para maximizar las recompensas acumuladas."
      ],
      "metadata": {
        "id": "wcVs_wbNwA8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo concreto:\n",
        "\n",
        "Vamos a adaptar el ejemplo a un **entorno de 3x3**, donde el agente tiene que llegar desde la posición inicial $[0,0]$ al destino final en $[2,2]$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Descripción del Entorno**\n",
        "1. **Espacio de estados:** Cada celda del mapa $ 3 \\times 3 $ representa un estado. Hay 9 estados en total, definidos por su posición $[x, y]$.\n",
        "2. **Acciones disponibles:** El agente puede moverse en cuatro direcciones:\n",
        "   - **Arriba $( \\text{UP} $)**: $[x-1, y]$ (si no está en el borde superior).\n",
        "   - **Abajo ($ \\text{DOWN} $)**: $[x+1, y]$ (si no está en el borde inferior).\n",
        "   - **Izquierda ($ \\text{LEFT} $)**: $[x, y-1]$ (si no está en el borde izquierdo).\n",
        "   - **Derecha ($ \\text{RIGHT} $)**: $[x, y+1]$ (si no está en el borde derecho).\n",
        "3. **Recompensas:**\n",
        "   - Llegar al destino $[2,2]$ otorga  $+10$.\n",
        "   - Cada movimiento normal tiene una recompensa de $-1$ (para incentivar un camino más corto).\n",
        "\n",
        "---\n",
        "\n",
        "### **Paso a Detallar: Actualización del Valor Q**\n",
        "\n",
        "Inicializamos la tabla Q para todos los estados y acciones con $ 0 $.\n",
        "\n",
        "| Estado/Acción | UP   | DOWN | LEFT | RIGHT |\n",
        "|---------------|-------|------|------|-------|\n",
        "| [0,0]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [0,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [0,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,0]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,0]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "\n",
        "#### **Ejemplo de Iteración**\n",
        "\n",
        "El agente comienza en el estado $[0,0]$. Vamos a detallar una actualización del valor $ Q $ para la acción **RIGHT** en este estado.\n",
        "\n",
        "1. **Paso 1: El agente elige la acción**  \n",
        "   Elige ir a la derecha (**RIGHT**), moviéndose al estado $[0,1]$.\n",
        "\n",
        "2. **Paso 2: Recibe la recompensa inmediata**  \n",
        "   Moverse cuesta $ -1 $, por lo que $ r = -1 $.\n",
        "\n",
        "3. **Paso 3: Identificar el estado siguiente $[0,1]$**  \n",
        "   Supongamos que todos los valores Q del estado $[0,1]$ aún son $ 0 $. Entonces:\n",
        "   $\n",
        "   \\max_{a'} Q([0,1], a') = 0\n",
        "   $\n",
        "\n",
        "4. **Paso 4: Actualizar $ Q([0,0], \\text{RIGHT}) $ usando la fórmula**\n",
        "   $\n",
        "   Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "   $\n",
        "   Sustituyendo valores:\n",
        "   - $ s = [0,0] $, $ a = \\text{RIGHT} $\n",
        "   - $ Q([0,0], \\text{RIGHT}) = 0 $ (inicialmente)\n",
        "   - $ r = -1 $\n",
        "   - $ \\gamma = 0.9 $ (factor de descuento)\n",
        "   - $ \\alpha = 0.1 $ (tasa de aprendizaje)\n",
        "\n",
        "   $\n",
        "   Q([0,0], \\text{RIGHT}) \\leftarrow 0 + 0.1 \\left[ -1 + 0.9 \\cdot 0 - 0 \\right]\n",
        "   $\n",
        "\n",
        "   $\n",
        "   Q([0,0], \\text{RIGHT}) \\leftarrow 0 + 0.1 \\cdot (-1) = -0.1\n",
        "   $\n",
        "\n",
        "5. **Paso 5: Actualizar la tabla Q**\n",
        "\n",
        "| Estado/Acción | UP   | DOWN | LEFT | RIGHT |\n",
        "|---------------|-------|------|------|-------|\n",
        "| [0,0]         | 0.0   | 0.0  | 0.0  | **-0.1** |\n",
        "| [0,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [0,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,0]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [1,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,0]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,1]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "| [2,2]         | 0.0   | 0.0  | 0.0  | 0.0   |\n",
        "\n",
        "---\n",
        "\n",
        "### **Notas Finales**\n",
        "1. Este proceso se repetirá mientras el agente explora el entorno.\n",
        "2. A medida que el agente encuentre el estado objetivo $[2,2]$ y reciba la recompensa $ +10 $, los valores Q de los estados anteriores comenzarán a reflejar el mejor camino hacia el objetivo.\n",
        "3. Con suficientes iteraciones, la tabla $ Q $ convergerá, permitiendo que el agente siga una política óptima para llegar al destino."
      ],
      "metadata": {
        "id": "Hw7JvlAzrLM1"
      }
    }
  ]
}